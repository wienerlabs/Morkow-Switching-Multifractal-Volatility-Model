"""Narrator — LLM-powered narrative reasoning engine for Cortex.

Translates deterministic risk model outputs into human-readable insights
using an OpenAI-compatible API (Ollama, Together, OpenAI). This is NOT a
template engine — every response is generated by an LLM call with full
context injection.

Four capabilities:
  1. explain_decision  — synthesize guardian + debate + news → trade narrative
  2. interpret_news    — LLM interprets news items for crypto-specific impact
  3. market_briefing   — periodic market state summary from all risk models
  4. answer_question   — interactive Q&A about the system's current state

Design principles:
  - Never in the hot path — advisory only, never blocks trading
  - All calls async with timeout + graceful fallback
  - NARRATOR_ENABLED=false (default) → zero side-effect, no LLM calls
  - Reads from existing stores (debate_store, news_buffer, guardian, regime)
"""

from __future__ import annotations

__all__ = [
    "explain_decision",
    "interpret_news",
    "market_briefing",
    "answer_question",
    "get_narrator_status",
]

import logging
import time
from typing import Any

from cortex.config import (
    NARRATOR_ENABLED,
    NARRATOR_BASE_URL,
    NARRATOR_API_KEY,
    NARRATOR_MODEL,
    NARRATOR_MAX_TOKENS,
    NARRATOR_TEMPERATURE,
    NARRATOR_TIMEOUT,
)

logger = logging.getLogger(__name__)

# ── Lazy LLM client (OpenAI-compatible) ───────────────────────────────

_client: Any = None
_call_count: int = 0
_error_count: int = 0
_total_latency_ms: float = 0.0


def _get_client():
    """Lazy-initialize async OpenAI-compatible client on first use."""
    global _client
    if _client is not None:
        return _client

    from openai import AsyncOpenAI
    _client = AsyncOpenAI(
        base_url=NARRATOR_BASE_URL,
        api_key=NARRATOR_API_KEY or "ollama",
        timeout=NARRATOR_TIMEOUT,
    )
    return _client


async def _llm_call(system_prompt: str, user_prompt: str) -> str:
    """Make an async LLM call via OpenAI API. Returns generated text.

    Tracks call count, errors, and latency for observability.
    """
    global _call_count, _error_count, _total_latency_ms

    client = _get_client()
    t0 = time.time()

    try:
        response = await client.chat.completions.create(
            model=NARRATOR_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            max_tokens=NARRATOR_MAX_TOKENS,
            temperature=NARRATOR_TEMPERATURE,
        )
        _call_count += 1
        _total_latency_ms += (time.time() - t0) * 1000
        return response.choices[0].message.content or ""
    except Exception as exc:
        _error_count += 1
        _total_latency_ms += (time.time() - t0) * 1000
        logger.error("Narrator LLM call failed: %s", exc, exc_info=True)
        raise


# ── Context Collectors ──────────────────────────────────────────────────

def _collect_guardian_context(assessment: dict[str, Any]) -> str:
    """Format a guardian assessment dict into LLM-readable context."""
    lines = [
        f"Risk Score: {assessment.get('risk_score', 'N/A')}/100",
        f"Approved: {assessment.get('approved', 'N/A')}",
        f"Regime State: {assessment.get('regime_state', 'N/A')}",
        f"Recommended Size: ${assessment.get('recommended_size', 0):,.2f}",
        f"Confidence: {assessment.get('confidence', 0):.2%}",
    ]

    veto = assessment.get("veto_reasons", [])
    if veto:
        lines.append(f"Veto Reasons: {', '.join(veto)}")

    threshold = assessment.get("effective_threshold", 75)
    lines.append(f"Approval Threshold: {threshold}")

    scores = assessment.get("component_scores", [])
    if scores:
        lines.append("\nComponent Scores:")
        for cs in scores:
            comp = cs.get("component", "?")
            score = cs.get("score", 0)
            lines.append(f"  - {comp}: {score:.1f}/100")

    cb = assessment.get("circuit_breaker")
    if cb:
        lines.append(f"\nCircuit Breakers: {cb}")

    limits = assessment.get("portfolio_limits")
    if limits and limits.get("blocked"):
        lines.append(f"Portfolio Limits BLOCKED: {limits.get('blockers', [])}")

    return "\n".join(lines)


def _collect_debate_context(debate: dict[str, Any]) -> str:
    """Format a debate result dict into LLM-readable context."""
    if not debate:
        return "No debate data available."

    lines = [
        f"Final Decision: {debate.get('final_decision', 'N/A')}",
        f"Confidence: {debate.get('final_confidence', 0):.2%}",
        f"Rounds: {debate.get('num_rounds', 0)}",
        f"Decision Changed: {debate.get('decision_changed', False)}",
        f"Recommended Size: {debate.get('recommended_size_pct', 0):.1%}",
    ]

    evidence = debate.get("evidence_summary", {})
    if evidence:
        lines.append(f"\nEvidence: {evidence.get('bullish', 0)} bullish, {evidence.get('bearish', 0)} bearish")

        for item in evidence.get("bullish_items", [])[:3]:
            lines.append(f"  + {item.get('claim', '')}")
        for item in evidence.get("bearish_items", [])[:3]:
            lines.append(f"  - {item.get('claim', '')}")

    rounds = debate.get("rounds", [])
    if rounds:
        last_round = rounds[-1]
        arb = last_round.get("arbitrator", {})
        lines.append(f"\nFinal Arbitrator:")
        lines.append(f"  Approval Score: {arb.get('approval_score', 0):.2%}")
        for r in arb.get("reasoning", []):
            lines.append(f"  → {r}")

    return "\n".join(lines)


def _collect_news_context(news_signal: dict[str, Any] | None, news_items: list[dict] | None = None) -> str:
    """Format news signal and items into LLM-readable context."""
    if not news_signal:
        return "No news data available."

    lines = [
        f"Sentiment EWMA: {news_signal.get('sentiment_ewma', 0):.4f}",
        f"Direction: {news_signal.get('direction', 'NEUTRAL')}",
        f"Strength: {news_signal.get('strength', 0):.4f}",
        f"Confidence: {news_signal.get('confidence', 0):.4f}",
        f"Entropy: {news_signal.get('entropy', 0):.4f}",
        f"Items: {news_signal.get('n_items', 0)} from {news_signal.get('n_sources', 0)} sources",
        f"Distribution: {news_signal.get('bull_pct', 0):.0%} bull / {news_signal.get('bear_pct', 0):.0%} bear / {news_signal.get('neutral_pct', 0):.0%} neutral",
    ]

    if news_items:
        lines.append("\nTop Headlines:")
        for item in news_items[:5]:
            sent = item.get("sentiment", {})
            lines.append(
                f"  [{sent.get('label', '?')}] {item.get('title', 'N/A')} "
                f"(impact={item.get('impact', 0)}, source={item.get('source', '?')})"
            )

    return "\n".join(lines)


def _collect_regime_context() -> str:
    """Collect current regime state from model stores (best-effort)."""
    lines = []
    try:
        from api.stores import msm_store
        tokens = list(msm_store.keys()) if hasattr(msm_store, "keys") else []
        if tokens:
            lines.append(f"Calibrated tokens: {', '.join(tokens[:10])}")
            # Get regime for first token as representative
            import numpy as np
            for token in tokens[:3]:
                data = msm_store.get(token)
                if data and "filter_probs" in data:
                    fp = data["filter_probs"]
                    pi_t = np.asarray(fp.iloc[-1] if hasattr(fp, "iloc") else fp[-1])
                    regime = int(np.argmax(pi_t)) + 1
                    crisis_prob = float(pi_t[-1]) if len(pi_t) > 0 else 0.0
                    lines.append(f"  {token}: regime={regime}, crisis_prob={crisis_prob:.4f}")
    except Exception:
        lines.append("Regime data unavailable.")

    return "\n".join(lines) if lines else "No regime data available."


# ── System Prompts ───────────────────────────────────────────────────────

EXPLAIN_SYSTEM = """You are the Cortex Risk Engine narrator. Your job is to translate
quantitative risk model outputs into clear, actionable trade narratives.

You have access to:
- Guardian composite risk assessment (EVT, SVJ, Hawkes, MSM regime, news, A-LAMS VaR)
- Adversarial debate transcripts (Trader vs Risk Manager vs Devil's Advocate → Portfolio Manager)
- News sentiment signals

Your output should:
1. Start with a clear APPROVE/REJECT recommendation with confidence level
2. Explain the key risk factors driving the decision
3. Note any model disagreements or surprising evidence
4. If debate changed the original decision, explain why
5. Suggest specific risk mitigations if applicable

Use precise numbers. Reference specific model outputs. Be direct — operators need
fast signal, not hedged prose. Write for a DeFi risk manager who understands
VaR, regime switching, and Hawkes processes."""

INTERPRET_NEWS_SYSTEM = """You are a crypto-native news analyst for the Cortex Risk Engine.
Your job is to go beyond keyword-based sentiment and provide LLM-powered interpretation
of news headlines and articles.

For each news batch you receive:
1. Identify the 2-3 most market-moving items and explain WHY they matter
2. Detect narratives that lexicon-based sentiment misses (e.g., regulatory nuance,
   protocol-specific context, second-order effects)
3. Assess sentiment DIRECTION (bullish/bearish/neutral) with reasoning
4. Estimate TIME HORIZON of impact (immediate, days, weeks)
5. Note any news that conflicts with the aggregate signal (contrarian items)

Be specific to crypto/DeFi. Reference Solana ecosystem context when relevant.
Output in structured prose, not bullet points."""

BRIEFING_SYSTEM = """You are the Cortex Risk Engine briefing officer. Generate concise
market briefings that synthesize all available risk model outputs into a single
coherent picture.

Your briefing should cover:
1. Overall market risk state (regime, VaR levels, key alerts)
2. News sentiment summary and key headlines
3. Model agreement/disagreement across components
4. Any active circuit breakers or veto triggers
5. Recommended operator actions

Write in a professional but direct style. Numbers matter — include specific scores,
probabilities, and thresholds. Target length: 200-400 words."""

QUESTION_SYSTEM = """You are the Cortex Risk Engine assistant. Answer operator questions
about the system's current state, risk models, and trading decisions.

You have access to live system context including:
- Guardian risk assessments and component scores
- MSM regime state and volatility levels
- News sentiment signals
- Debate transcripts and decision history
- Circuit breaker states

Answer precisely based on the provided context. If you don't have enough data
to answer, say so clearly. Never fabricate numbers. Reference specific model
outputs and scores when relevant."""


# ── Public API ─────────────────────────────────────────────────────────

async def explain_decision(
    assessment: dict[str, Any],
    token: str = "",
    direction: str = "",
    trade_size_usd: float = 0.0,
) -> dict[str, Any]:
    """Generate LLM narrative for a guardian trade assessment.

    Args:
        assessment: Full guardian assess_trade() result dict.
        token: Token symbol.
        direction: Trade direction (long/short).
        trade_size_usd: Proposed trade size.

    Returns:
        Dict with 'narrative', 'model', 'latency_ms', etc.
    """
    if not NARRATOR_ENABLED:
        return {"enabled": False, "narrative": None}

    t0 = time.time()

    guardian_ctx = _collect_guardian_context(assessment)
    debate_ctx = _collect_debate_context(assessment.get("debate") or {})
    regime_ctx = _collect_regime_context()

    # Get news from buffer
    news_ctx = "No news data available."
    try:
        from cortex.news import news_buffer
        signal = news_buffer.get_signal()
        full = news_buffer.get_full(max_items=5)
        items = full.get("items", []) if full else None
        news_ctx = _collect_news_context(signal, items)
    except Exception:
        pass

    user_prompt = f"""Explain this trade decision:

TOKEN: {token}
DIRECTION: {direction}
SIZE: ${trade_size_usd:,.2f}

=== GUARDIAN ASSESSMENT ===
{guardian_ctx}

=== ADVERSARIAL DEBATE ===
{debate_ctx}

=== NEWS SIGNAL ===
{news_ctx}

=== REGIME STATE ===
{regime_ctx}"""

    try:
        narrative = await _llm_call(EXPLAIN_SYSTEM, user_prompt)
        latency_ms = round((time.time() - t0) * 1000, 1)
        logger.info("Narrator explain_decision: %s %s %dms", token, direction, latency_ms)
        return {
            "enabled": True,
            "narrative": narrative,
            "model": NARRATOR_MODEL,
            "latency_ms": latency_ms,
            "token": token,
            "direction": direction,
        }
    except Exception as exc:
        latency_ms = round((time.time() - t0) * 1000, 1)
        logger.error("Narrator explain_decision failed: %s", exc)
        return {
            "enabled": True,
            "narrative": None,
            "error": str(exc),
            "latency_ms": latency_ms,
        }


async def interpret_news(
    news_items: list[dict[str, Any]] | None = None,
    news_signal: dict[str, Any] | None = None,
) -> dict[str, Any]:
    """LLM-powered interpretation of news items beyond lexicon matching.

    Args:
        news_items: List of news item dicts. If None, reads from news_buffer.
        news_signal: Aggregate signal dict. If None, reads from news_buffer.

    Returns:
        Dict with 'interpretation', 'model', 'latency_ms', etc.
    """
    if not NARRATOR_ENABLED:
        return {"enabled": False, "interpretation": None}

    t0 = time.time()

    # Fallback to buffer if not provided
    if news_items is None or news_signal is None:
        try:
            from cortex.news import news_buffer
            full = news_buffer.get_full(max_items=10)
            if full:
                news_items = news_items or full.get("items", [])
                news_signal = news_signal or full.get("signal")
        except Exception:
            pass

    if not news_items:
        return {
            "enabled": True,
            "interpretation": None,
            "error": "No news items available to interpret.",
            "latency_ms": 0,
        }

    news_ctx = _collect_news_context(news_signal, news_items)
    regime_ctx = _collect_regime_context()

    user_prompt = f"""Interpret this crypto news batch:

=== AGGREGATE SIGNAL ===
{news_ctx}

=== CURRENT REGIME ===
{regime_ctx}

Provide your analysis of what these headlines actually mean for the market,
especially anything the lexicon-based sentiment system might miss."""

    try:
        interpretation = await _llm_call(INTERPRET_NEWS_SYSTEM, user_prompt)
        latency_ms = round((time.time() - t0) * 1000, 1)
        logger.info("Narrator interpret_news: %d items, %dms", len(news_items), latency_ms)
        return {
            "enabled": True,
            "interpretation": interpretation,
            "model": NARRATOR_MODEL,
            "latency_ms": latency_ms,
            "n_items": len(news_items),
        }
    except Exception as exc:
        latency_ms = round((time.time() - t0) * 1000, 1)
        logger.error("Narrator interpret_news failed: %s", exc)
        return {
            "enabled": True,
            "interpretation": None,
            "error": str(exc),
            "latency_ms": latency_ms,
        }


async def market_briefing() -> dict[str, Any]:
    """Generate a comprehensive market briefing from all risk model outputs.

    Reads live data from:
    - News buffer (sentiment signal)
    - Debate store (recent debate decisions)
    - Regime state (MSM model stores)
    - Circuit breaker states

    Returns:
        Dict with 'briefing', 'model', 'latency_ms', etc.
    """
    if not NARRATOR_ENABLED:
        return {"enabled": False, "briefing": None}

    t0 = time.time()

    # Collect all available context
    regime_ctx = _collect_regime_context()

    news_ctx = "No news data available."
    try:
        from cortex.news import news_buffer
        signal = news_buffer.get_signal()
        full = news_buffer.get_full(max_items=5)
        items = full.get("items", []) if full else None
        news_ctx = _collect_news_context(signal, items)
    except Exception:
        pass

    debate_ctx = "No recent debates."
    try:
        from cortex.debate_store import get_debate_store
        store = get_debate_store()
        stats = store.get_decision_stats(hours=24.0)
        recent = store.get_recent(limit=3)
        lines = [
            f"Last 24h: {stats.get('total_debates', 0)} debates",
            f"Decisions: {stats.get('decisions', {})}",
            f"Avg Confidence: {stats.get('avg_confidence', 0):.2%}",
            f"Decision Changed: {stats.get('decision_changed_count', 0)} times",
        ]
        if recent:
            lines.append("\nMost Recent Debates:")
            for d in recent[:3]:
                lines.append(
                    f"  {d.get('token', '?')} {d.get('direction', '?')}: "
                    f"{d.get('final_decision', '?')} (conf={d.get('final_confidence', 0):.2%})"
                )
        debate_ctx = "\n".join(lines)
    except Exception:
        pass

    cb_ctx = "No circuit breaker data."
    try:
        from cortex.circuit_breaker import get_all_states
        states = get_all_states()
        if states:
            lines = []
            for s in states:
                lines.append(f"  {s.get('name', '?')}: {s.get('state', '?')} (fails={s.get('fail_count', 0)})")
            cb_ctx = "Circuit Breakers:\n" + "\n".join(lines)
    except Exception:
        pass

    kelly_ctx = "No Kelly data."
    try:
        from cortex.guardian import get_kelly_stats
        kelly = get_kelly_stats()
        if kelly.get("active"):
            kelly_ctx = (
                f"Kelly: active, win_rate={kelly.get('win_rate', 0):.2%}, "
                f"kelly_fraction={kelly.get('kelly_fraction', 0):.4f}, "
                f"n_trades={kelly.get('n_trades', 0)}"
            )
        else:
            kelly_ctx = f"Kelly: inactive ({kelly.get('reason', 'unknown')})"
    except Exception:
        pass

    user_prompt = f"""Generate a market briefing from the following risk engine state:

=== REGIME STATE ===
{regime_ctx}

=== NEWS SIGNAL ===
{news_ctx}

=== DEBATE HISTORY (24h) ===
{debate_ctx}

=== CIRCUIT BREAKERS ===
{cb_ctx}

=== KELLY CRITERION ===
{kelly_ctx}"""

    try:
        briefing = await _llm_call(BRIEFING_SYSTEM, user_prompt)
        latency_ms = round((time.time() - t0) * 1000, 1)
        logger.info("Narrator market_briefing: %dms", latency_ms)
        return {
            "enabled": True,
            "briefing": briefing,
            "model": NARRATOR_MODEL,
            "latency_ms": latency_ms,
        }
    except Exception as exc:
        latency_ms = round((time.time() - t0) * 1000, 1)
        logger.error("Narrator market_briefing failed: %s", exc)
        return {
            "enabled": True,
            "briefing": None,
            "error": str(exc),
            "latency_ms": latency_ms,
        }


async def answer_question(
    question: str,
    context_overrides: dict[str, Any] | None = None,
) -> dict[str, Any]:
    """Answer an operator question about the system's current state.

    Args:
        question: Free-text question from the operator.
        context_overrides: Optional extra context to inject (e.g., specific assessment).

    Returns:
        Dict with 'answer', 'model', 'latency_ms', etc.
    """
    if not NARRATOR_ENABLED:
        return {"enabled": False, "answer": None}

    t0 = time.time()

    # Gather live context
    regime_ctx = _collect_regime_context()

    news_ctx = "No news data available."
    try:
        from cortex.news import news_buffer
        signal = news_buffer.get_signal()
        news_ctx = _collect_news_context(signal)
    except Exception:
        pass

    kelly_ctx = "No Kelly data."
    try:
        from cortex.guardian import get_kelly_stats
        kelly = get_kelly_stats()
        kelly_ctx = str(kelly)
    except Exception:
        pass

    extra = ""
    if context_overrides:
        extra = f"\n=== ADDITIONAL CONTEXT ===\n{context_overrides}"

    user_prompt = f"""Operator Question: {question}

=== LIVE SYSTEM STATE ===

REGIME:
{regime_ctx}

NEWS:
{news_ctx}

KELLY:
{kelly_ctx}
{extra}"""

    try:
        answer = await _llm_call(QUESTION_SYSTEM, user_prompt)
        latency_ms = round((time.time() - t0) * 1000, 1)
        logger.info("Narrator answer_question: %dms", latency_ms)
        return {
            "enabled": True,
            "answer": answer,
            "model": NARRATOR_MODEL,
            "latency_ms": latency_ms,
            "question": question,
        }
    except Exception as exc:
        latency_ms = round((time.time() - t0) * 1000, 1)
        logger.error("Narrator answer_question failed: %s", exc)
        return {
            "enabled": True,
            "answer": None,
            "error": str(exc),
            "latency_ms": latency_ms,
        }


def get_narrator_status() -> dict[str, Any]:
    """Return narrator health and usage stats."""
    avg_latency = round(_total_latency_ms / _call_count, 1) if _call_count > 0 else 0
    return {
        "enabled": NARRATOR_ENABLED,
        "model": NARRATOR_MODEL,
        "api_key_set": bool(NARRATOR_API_KEY),
        "call_count": _call_count,
        "error_count": _error_count,
        "avg_latency_ms": avg_latency,
        "total_latency_ms": round(_total_latency_ms, 1),
    }
